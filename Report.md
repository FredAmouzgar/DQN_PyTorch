# Deep Q-Network (DQN) implemetation Report

### Introduction
This repository is an implementation of the Deep Q-Network (DQN) algorithm for the Banana Environment developed by Unity3D and accessed through the UnityEnvironment library. It is an extension of the code sample provided by the Udacity Deep RL teaching crew (for more information visit their [website](https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893)). The environment is presented as a vector; thus, we did not use Convolutional Neural Networks (CNN) \[[1](http://yann.lecun.com/exdb/publis/pdf/lecun-99.pdf)\] in the implementation.

The Deep Q-Network as a combination of deep neural networks and Q-learning was defined in the context of games \[[2](https://arxiv.org/abs/1312.5602)\], with its input being a sequence of images from a game. Q-learning \[[3](https://link.springer.com/article/10.1007%2FBF00992698)\] as a temporal difference method initializes and updates Q-values based on the TD error, <img src="https://render.githubusercontent.com/render/math?math=Q(s,a) \leftarrow Q(s,a) %2B \alpha [R %2B \gamma \max Q(s',a') - Q(s,a)]">. The agent actions are chosen by a random process called <img src="https://render.githubusercontent.com/render/math?math=\epsilon">-greedy. <img src="https://render.githubusercontent.com/render/math?math=\epsilon">-greedy balances the exploration-exploitation to facilitate a better learning outcome. This process continues until convergence.

The DQN paper proposed five-layer neural network architecture. The input to the network is the environment state, and its output is the set of Q-values for each action. When a vectorized environment is used, the CNN part can be cautiously removed, utilizing the provided vectors as features generated by the CNN. The TD error is the basis for computing the loss function and training the network, <img src="https://render.githubusercontent.com/render/math?math=L(\theta) \leftarrow (r %2B \gamma \max Q(s', a':\theta^{-}) - Q(s,a:\theta))^2">. DQN's success can be attributed to two main innovative techniques:
1. Experience Replay
2. Target Networks

### Method
DQN, as represented in Figure 1, executes a typical reinforcement learning algorithm. It gathers a repository of experiences or transitions while exploring the environment. This dataset is collected by a behavior policy which is being updated more regularly. The target policy, which determines the final policy of the agent, is updated on a slower rate. Learning in a discrete action environment is classification.

<center><img src="https://raw.githubusercontent.com/FredAmouzgar/DQN_PyTorch/master/pics/DQN_algorithm.png" width="800" height="400">
<br><font size=2>Figure 1: The DQN <a href="https://arxiv.org/abs/1312.5602">[2]</a></font></center>

### The Banana Environment
The Banana environment is a vectorized version of the Banana Collection designed by the Unity game engine. The task is simple. The agent explores its world consists of Yellow (good) and Purple (bad) bananas. It should absorb the good ones (by walking on them), and avoid the bad ones. Having a good banana is rewarded with a `+1` reward, and a bad one with `-1`. In this activity, we consider any agent capable of achieving an average reward of more than 13, a successful agent.

The action space has four discrete actions:
1. Move Forward (action 0)
2. Move Backward (action 1)
3. Turn Right (action 2)
4. Turn Left (action 3)

### Experiments
This repository consists of these files:

*These files are saved under the "src" directory.*
1. <ins> model.py </ins>: This module provides the underlying neural network for our agent. When we train our agent, this neural network is going to be updated by backpropagation.
2. <ins>buffer.py</ins>: This module implements the "memory" of our agent, also known as the Experience Replay.
3. <ins>agent.py</ins>: This is the body of our agent. It implements the way the agent acts (using epsilon-greedy policy), and learn an optimal policy.
4. <ins>train.py</ins>: This module has the train function which takes the agent, the environment, number of training episodes and the required hyper-parameters and trains the agent accordingly.

To test the code, after cloning the project, open the `Navigation.ipynb` notebook. It has all the necessary steps to install and load the packages, and train and test the agent. It also automatically detects the operating system and loads the corresponding environment. There is an already trained agent stored in `checkpoint.pth`, by running the last part of the notebook, this can be directly tested.

Figure 2 is depicted a reward plot acquired by the agent while learning. It surpasses +16 after around 1100 episodes.
<center><img src="https://github.com/FredAmouzgar/DQN_PyTorch/raw/master/pics/DQN_reward_plot.png" width="400" height="200">
<br><font size=2>Figure 2: The average reward during training</font></center>

Figure 3 shows one episode after training.

<center><Img src="https://github.com/FredAmouzgar/DQN_PyTorch/raw/master/pics/BananaAgent.gif" width="400" height="200">
<br><font size=2>Figure 3: A Trained Agent</font></center>

### Hyperparameters
Banana Environment:
- State size: 37
- Action size: 4


DQN Agent:
- Replay memory buffer size: <img src="https://render.githubusercontent.com/render/math?math=10 ^ 5">
- Batch size: 64
- <img src="https://render.githubusercontent.com/render/math?math=\gamma">: 0.99
- <img src="https://render.githubusercontent.com/render/math?math=\tau"> for target update: <img src="https://render.githubusercontent.com/render/math?math=10^{-3}">
- <img src="https://render.githubusercontent.com/render/math?math=\alpha"> or learning rate: <img src="https://render.githubusercontent.com/render/math?math=5\times 10 ^ {-4}">

Neural Networks:
- Input Layer: 37 (state size)
- Layer 1: 64 units
- Layer 2: 64 units
- Output Layer: 4 (action size)
- Activation function: ReLU, linear (output layer)

Training Parameters:
- Episodes: 1200
- Steps in every episode: 1000
- First <img src="https://render.githubusercontent.com/render/math?math=\epsilon"> value: 1.0
- Final <img src="https://render.githubusercontent.com/render/math?math=\epsilon"> value: 0.01
- <img src="https://render.githubusercontent.com/render/math?math=\epsilon"> decay: 0.995

### Future Work:
1. Augmenting the agent with prioritized experience replay buffer
2. Double DQN
3. Dueling DQN
4. Putting it all together and implementing the rainbow algorithm.